{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a41b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Common setup: paths for models and results\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "MODELS_DIR = Path(\"trained_models\")\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921e990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "loading annotations into memory...\n",
      "Done (t=4.08s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n",
      "Evaluating FRCNN before training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval FRCNN (COCO mAP): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Initial FRCNN metrics: {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train FRCNN: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:23<00:00,  3.01it/s]\n",
      "Eval FRCNN (COCO mAP): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 16.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.11s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.07s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.040\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.023\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.015\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.036\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.040\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.044\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.065\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.044\n",
      "[FRCNN] Epoch 1/3 | train_loss=1.2295 | AP=0.0266 | AP50=0.0540 | AP75=0.0244 | APs=0.0339 | APm=0.0402 | APl=0.0234\n",
      "  -> New best FRCNN model saved (AP=0.0266)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train FRCNN: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:23<00:00,  2.98it/s]\n",
      "Eval FRCNN (COCO mAP): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.18s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.10s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.115\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.246\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.092\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.105\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.168\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.109\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.096\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.176\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.187\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.143\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.232\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.204\n",
      "[FRCNN] Epoch 2/3 | train_loss=0.9675 | AP=0.1155 | AP50=0.2457 | AP75=0.0921 | APs=0.1046 | APm=0.1680 | APl=0.1092\n",
      "  -> New best FRCNN model saved (AP=0.1155)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train FRCNN: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:22<00:00,  3.02it/s]\n",
      "Eval FRCNN (COCO mAP): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.18s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.10s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.338\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.111\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.119\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.215\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.131\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.240\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.262\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.166\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.314\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.347\n",
      "[FRCNN] Epoch 3/3 | train_loss=0.7763 | AP=0.1537 | AP50=0.3383 | AP75=0.1107 | APs=0.1190 | APm=0.2077 | APl=0.2146\n",
      "  -> New best FRCNN model saved (AP=0.1537)\n",
      "Saved FRCNN history to results/frcnn_history.json\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# ======================================\n",
    "# 1. Faster R-CNN: train + COCO mAP\n",
    "# ======================================\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "from src.utils.coco_utils import make_coco_loaders\n",
    "from src.models.frcnn import get_frcnn_model\n",
    "\n",
    "\n",
    "def get_coco_api_from_loader(loader):\n",
    "    \"\"\"\n",
    "    Robustly get the underlying pycocotools COCO object from a DataLoader.\n",
    "    \"\"\"\n",
    "    ds = loader.dataset\n",
    "    for _ in range(10):\n",
    "        if hasattr(ds, \"coco\"):\n",
    "            return ds.coco\n",
    "        if hasattr(ds, \"dataset\"):\n",
    "            ds = ds.dataset\n",
    "        else:\n",
    "            break\n",
    "    raise AttributeError(\n",
    "        \"Could not find 'coco' attribute in dataset. \"\n",
    "        \"Please check make_coco_loaders implementation.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_img_ids_for_loader(loader):\n",
    "    \"\"\"\n",
    "    Build a list img_ids such that:\n",
    "        img_ids[i] = COCO image_id corresponding to loader.dataset[i]\n",
    "\n",
    "    Handles Subset and simple wrappers. Assumes val_loader has shuffle=False.\n",
    "    \"\"\"\n",
    "    ds = loader.dataset\n",
    "    indices = None  # indices in the base dataset\n",
    "\n",
    "    while True:\n",
    "        if isinstance(ds, Subset):\n",
    "            if indices is None:\n",
    "                indices = list(ds.indices)\n",
    "            else:\n",
    "                indices = [indices[i] for i in ds.indices]\n",
    "            ds = ds.dataset\n",
    "            continue\n",
    "\n",
    "        if not hasattr(ds, \"coco\") and hasattr(ds, \"dataset\"):\n",
    "            ds = ds.dataset\n",
    "            continue\n",
    "\n",
    "        break\n",
    "\n",
    "    if not hasattr(ds, \"coco\"):\n",
    "        raise RuntimeError(\n",
    "            \"Could not find a base COCO dataset with a 'coco' attribute under loader.dataset\"\n",
    "        )\n",
    "\n",
    "    base_ds = ds\n",
    "    coco = base_ds.coco\n",
    "\n",
    "    if indices is None:\n",
    "        indices = list(range(len(base_ds)))\n",
    "\n",
    "    if hasattr(base_ds, \"ids\"):\n",
    "        base_img_ids = list(base_ds.ids)\n",
    "    else:\n",
    "        base_img_ids = list(sorted(coco.getImgIds()))\n",
    "\n",
    "    loader_img_ids = [int(base_img_ids[i]) for i in indices]\n",
    "    return loader_img_ids\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in tqdm(loader, desc=\"Train FRCNN\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += losses.item()\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_coco_mAP(model, loader, device):\n",
    "    \"\"\"\n",
    "    Run COCO-style evaluation on *your subset* of val2017.\n",
    "    Returns dict with AP, AP50, AP75, APs, APm, APl.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    coco = get_coco_api_from_loader(loader)\n",
    "    coco_img_ids_all = set(coco.getImgIds())\n",
    "\n",
    "    loader_img_ids = get_img_ids_for_loader(loader)\n",
    "    assert len(loader_img_ids) == len(loader.dataset), \\\n",
    "        \"Length mismatch between loader_img_ids and loader.dataset\"\n",
    "\n",
    "    results = []\n",
    "    global_idx = 0\n",
    "\n",
    "    for images, _targets in tqdm(loader, desc=\"Eval FRCNN (COCO mAP)\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        batch_size = len(outputs)\n",
    "        batch_img_ids = loader_img_ids[global_idx: global_idx + batch_size]\n",
    "        global_idx += batch_size\n",
    "\n",
    "        for img_id, output in zip(batch_img_ids, outputs):\n",
    "            if img_id not in coco_img_ids_all:\n",
    "                continue\n",
    "\n",
    "            boxes = output[\"boxes\"].detach().cpu()\n",
    "            scores = output[\"scores\"].detach().cpu()\n",
    "            labels = output[\"labels\"].detach().cpu()\n",
    "\n",
    "            if boxes.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # xyxy -> xywh\n",
    "            boxes_xywh = boxes.clone()\n",
    "            boxes_xywh[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "            boxes_xywh[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "            for box, score, label in zip(boxes_xywh, scores, labels):\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"image_id\": int(img_id),\n",
    "                        \"category_id\": int(label),  # COCO cat_ids\n",
    "                        \"bbox\": box.tolist(),\n",
    "                        \"score\": float(score),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if not results:\n",
    "        print(\"No detections to evaluate.\")\n",
    "        return None\n",
    "\n",
    "    coco_dt = coco.loadRes(results)\n",
    "    coco_eval = COCOeval(coco, coco_dt, iouType=\"bbox\")\n",
    "\n",
    "    eval_img_ids = sorted({r[\"image_id\"] for r in results})\n",
    "    coco_eval.params.imgIds = eval_img_ids\n",
    "\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    metrics = {\n",
    "        \"AP\":   float(coco_eval.stats[0]),\n",
    "        \"AP50\": float(coco_eval.stats[1]),\n",
    "        \"AP75\": float(coco_eval.stats[2]),\n",
    "        \"APs\":  float(coco_eval.stats[3]),\n",
    "        \"APm\":  float(coco_eval.stats[4]),\n",
    "        \"APl\":  float(coco_eval.stats[5]),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ------------ main FRCNN script ------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_loader, val_loader = make_coco_loaders(\n",
    "    root=\"/mnt/ssd2/santana-coco/data/coco\",\n",
    "    batch_size=2,\n",
    "    num_workers=4,\n",
    "    train_limit=500,   # None for full train2017 per proposal\n",
    "    val_limit=100,     # None for full val2017 per proposal\n",
    ")\n",
    "\n",
    "num_classes = 91  # standard COCO setting (incl. background)\n",
    "model = get_frcnn_model(num_classes=num_classes).to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "num_epochs = 3\n",
    "best_ap = 0.0\n",
    "\n",
    "frcnn_history = []\n",
    "\n",
    "# Evaluate pre-trained FRCNN before fine-tuning\n",
    "print(\"Evaluating FRCNN before training...\")\n",
    "metrics0 = evaluate_coco_mAP(model, val_loader, device)\n",
    "print(\"Initial FRCNN metrics:\", metrics0)\n",
    "\n",
    "entry0 = {\"epoch\": 0, \"train_loss\": None}\n",
    "if metrics0 is not None:\n",
    "    entry0.update(metrics0)\n",
    "frcnn_history.append(entry0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    metrics = evaluate_coco_mAP(model, val_loader, device)\n",
    "    if metrics is not None:\n",
    "        ap = metrics[\"AP\"]\n",
    "        print(\n",
    "            f\"[FRCNN] Epoch {epoch + 1}/{num_epochs} | \"\n",
    "            f\"train_loss={train_loss:.4f} | \"\n",
    "            f\"AP={ap:.4f} | AP50={metrics['AP50']:.4f} | \"\n",
    "            f\"AP75={metrics['AP75']:.4f} | APs={metrics['APs']:.4f} | \"\n",
    "            f\"APm={metrics['APm']:.4f} | APl={metrics['APl']:.4f}\"\n",
    "        )\n",
    "\n",
    "        history_entry = {\"epoch\": epoch + 1, \"train_loss\": train_loss}\n",
    "        history_entry.update(metrics)\n",
    "        frcnn_history.append(history_entry)\n",
    "\n",
    "        if ap > best_ap:\n",
    "            best_ap = ap\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                MODELS_DIR / \"frcnn_best.pth\",\n",
    "            )\n",
    "            print(f\"  -> New best FRCNN model saved (AP={ap:.4f})\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"[FRCNN] Epoch {epoch + 1}/{num_epochs} | \"\n",
    "            f\"train_loss={train_loss:.4f} | no detections on val set\"\n",
    "        )\n",
    "        frcnn_history.append(\n",
    "            {\"epoch\": epoch + 1, \"train_loss\": train_loss}\n",
    "        )\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        MODELS_DIR / f\"frcnn_epoch{epoch + 1}.pth\",\n",
    "    )\n",
    "\n",
    "# Save FRCNN history for later plotting\n",
    "with open(RESULTS_DIR / \"frcnn_history.json\", \"w\") as f:\n",
    "    json.dump(frcnn_history, f, indent=2)\n",
    "print(\"Saved FRCNN history to\", RESULTS_DIR / \"frcnn_history.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868b888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating YOLOv8s (pretrained) on coco_subset val...\n",
      "Ultralytics 8.3.231 üöÄ Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4060, 7814MiB)\n",
      "YOLOv8s summary (fused): 72 layers, 11,156,544 parameters, 0 gradients, 28.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 7331.3¬±1356.6 MB/s, size: 169.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/ssd2/santana-coco/data/coco_subset_500_100/labels/val.cache... 100 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 100/100 463.5Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7/7 8.3it/s 0.8s0.1s\n",
      "                   all        100        860      0.719      0.636      0.709      0.557\n",
      "                person         54        288      0.869      0.625      0.792      0.558\n",
      "               bicycle          2          2      0.644          1      0.995      0.672\n",
      "                   car         11         32       0.86      0.383      0.685      0.489\n",
      "            motorcycle          1          2      0.635        0.5      0.497      0.448\n",
      "              airplane          3          4          1      0.747      0.995        0.8\n",
      "                   bus          3          5      0.878          1      0.995      0.956\n",
      "                 train          4          5      0.899          1      0.995      0.882\n",
      "                 truck          5         11      0.379      0.225      0.515      0.378\n",
      "                  boat          1          1      0.684          1      0.995      0.304\n",
      "         traffic light          4         10      0.724      0.528      0.723      0.425\n",
      "          fire hydrant          1          1      0.752          1      0.995      0.895\n",
      "             stop sign          1          1      0.712          1      0.995      0.895\n",
      "         parking meter          1          1      0.332          1      0.497      0.448\n",
      "                 bench          5          9          1      0.314      0.414      0.267\n",
      "                  bird          5         24      0.912      0.867      0.872      0.635\n",
      "                   cat          3          3      0.585      0.667      0.485      0.413\n",
      "                   dog          4          4      0.965          1      0.995      0.867\n",
      "                 horse          2          6      0.851        0.5      0.599      0.543\n",
      "                 sheep          2          7          0          0     0.0152    0.00815\n",
      "                   cow          4         15      0.856      0.793      0.845      0.639\n",
      "              elephant          1          1        0.7          1      0.995      0.995\n",
      "                  bear          1          1      0.735          1      0.995      0.995\n",
      "                 zebra          2          5          1      0.912      0.995      0.797\n",
      "               giraffe          2         15      0.872      0.912      0.943      0.557\n",
      "              backpack          5          9      0.665      0.221      0.379      0.148\n",
      "              umbrella          3          3      0.682          1      0.913      0.608\n",
      "               handbag          5          9       0.19     0.0422      0.199     0.0969\n",
      "                   tie          1          1          0          0     0.0195    0.00975\n",
      "              suitcase          1          5      0.802       0.81      0.928      0.604\n",
      "               frisbee          1          1      0.916          1      0.995      0.895\n",
      "                  skis          1          4      0.822        0.5      0.549      0.459\n",
      "           sports ball          7          8      0.628      0.428      0.532      0.327\n",
      "                  kite          4         16          1      0.272      0.464      0.347\n",
      "          baseball bat          5          9      0.786      0.414      0.461      0.297\n",
      "        baseball glove          5          7      0.904      0.714      0.801      0.566\n",
      "             surfboard          4          5      0.796        0.8      0.752      0.625\n",
      "         tennis racket          2          3      0.669      0.675      0.913      0.733\n",
      "                bottle         12         35      0.904       0.54      0.706      0.461\n",
      "            wine glass          4          7      0.628      0.429      0.365      0.238\n",
      "                   cup          9         26      0.834      0.654      0.795      0.604\n",
      "                  fork          1          2      0.658        0.5      0.533      0.514\n",
      "                 knife          3          3       0.77      0.667      0.715      0.605\n",
      "                 spoon          7         12      0.781      0.167        0.4       0.21\n",
      "                  bowl         10         18      0.729      0.448      0.734      0.514\n",
      "                banana          4         17      0.476      0.294       0.52       0.39\n",
      "                 apple          1          4          1      0.436      0.771      0.685\n",
      "              sandwich          1          2      0.356          1      0.995      0.796\n",
      "                orange          1          1      0.736          1      0.995      0.895\n",
      "              broccoli          1          2          1      0.777      0.995      0.746\n",
      "                carrot          2          5      0.383        0.2      0.191      0.128\n",
      "               hot dog          4         10      0.952        0.6       0.71      0.543\n",
      "                 pizza          5          5          1      0.595      0.833      0.584\n",
      "                 donut          1          1      0.165          1      0.995      0.995\n",
      "                  cake          5         11      0.469      0.455      0.531      0.457\n",
      "                 chair         15         57      0.766      0.368      0.553      0.368\n",
      "                 couch          4          4      0.892          1      0.995      0.785\n",
      "          potted plant          6          9      0.337      0.111       0.24      0.132\n",
      "                   bed          3          3      0.719      0.667      0.755      0.689\n",
      "          dining table         10         14      0.525      0.475      0.604       0.53\n",
      "                toilet          3          3      0.827          1      0.995      0.897\n",
      "                    tv          5          7          1       0.38       0.52      0.437\n",
      "                laptop          3          5      0.755        0.8      0.886       0.82\n",
      "                 mouse          3          3       0.62      0.574      0.452      0.419\n",
      "                remote          3          6      0.848        0.5      0.667      0.513\n",
      "              keyboard          3          4          1      0.448      0.912      0.815\n",
      "            cell phone          6          7      0.747      0.286      0.398      0.274\n",
      "             microwave          3          3      0.806          1      0.995       0.93\n",
      "                  oven          5          7      0.932      0.571      0.624      0.357\n",
      "               toaster          1          1      0.784          1      0.995      0.697\n",
      "                  sink          6          7      0.639      0.571      0.778      0.481\n",
      "          refrigerator          4         12          1      0.292      0.523      0.353\n",
      "                  book          2          2      0.402        0.5       0.39      0.327\n",
      "                 clock          2          2      0.765          1      0.995      0.895\n",
      "                  vase          6          7      0.421      0.286      0.452      0.337\n",
      "              scissors          1          1      0.707          1      0.995      0.995\n",
      "            teddy bear          2          2      0.621      0.864      0.663      0.351\n",
      "Speed: 0.9ms preprocess, 4.7ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/home/santana/MyStuff/assignments/dl-final-project/runs/detect/val4\u001b[0m\n",
      "YOLOv8s initial: {'stage': 'pretrained', 'map': 0.55729558106293, 'map50': 0.709244499649114, 'map75': 0.6039374027530552}\n",
      "New https://pypi.org/project/ultralytics/8.3.233 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.231 üöÄ Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4060, 7814MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data/coco_subset.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8s_640_subset2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=trained_models/yolo, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/santana/MyStuff/assignments/dl-final-project/trained_models/yolo/yolov8s_640_subset2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2147008  ultralytics.nn.modules.head.Detect           [80, [128, 256, 512]]         \n",
      "Model summary: 129 layers, 11,166,560 parameters, 11,166,544 gradients, 28.8 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 125.7¬±98.2 MB/s, size: 94.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/ssd2/santana-coco/data/coco_subset_500_100/labels/train.cache... 500 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 500/500 505.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 4953.0¬±2778.4 MB/s, size: 126.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/ssd2/santana-coco/data/coco_subset_500_100/labels/val.cache... 100 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 100/100 563.8Kit/s 0.0s\n",
      "Plotting labels to /home/santana/MyStuff/assignments/dl-final-project/trained_models/yolo/yolov8s_640_subset2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/santana/MyStuff/assignments/dl-final-project/trained_models/yolo/yolov8s_640_subset2\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/3      3.79G      2.845      4.374      2.334         80        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 5.0it/s 6.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4/4 7.0it/s 0.6s0.3s\n",
      "                   all        100        860      0.027    0.00912    0.00545    0.00324\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/3      4.01G      2.573      4.096      2.146         95        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 5.5it/s 5.8s0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4/4 8.6it/s 0.5s0.2s\n",
      "                   all        100        860     0.0627     0.0325     0.0468      0.025\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        3/3      3.85G      2.412      3.963      2.061         40        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 5.5it/s 5.8s0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4/4 8.9it/s 0.5s0.2s\n",
      "                   all        100        860      0.106     0.0567     0.0828     0.0451\n",
      "\n",
      "3 epochs completed in 0.006 hours.\n",
      "Optimizer stripped from /home/santana/MyStuff/assignments/dl-final-project/trained_models/yolo/yolov8s_640_subset2/weights/last.pt, 22.6MB\n",
      "Optimizer stripped from /home/santana/MyStuff/assignments/dl-final-project/trained_models/yolo/yolov8s_640_subset2/weights/best.pt, 22.6MB\n",
      "\n",
      "Validating /home/santana/MyStuff/assignments/dl-final-project/trained_models/yolo/yolov8s_640_subset2/weights/best.pt...\n",
      "Ultralytics 8.3.231 üöÄ Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4060, 7814MiB)\n",
      "Model summary (fused): 72 layers, 11,156,544 parameters, 0 gradients, 28.6 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4/4 7.5it/s 0.5s0.2s\n",
      "                   all        100        860      0.106     0.0567     0.0828     0.0451\n",
      "                person         54        288    0.00774      0.743       0.43      0.225\n",
      "               bicycle          2          2          0          0          0          0\n",
      "                   car         11         32      0.667      0.125      0.405      0.199\n",
      "            motorcycle          1          2          0          0          0          0\n",
      "              airplane          3          4          0          0          0          0\n",
      "                   bus          3          5        0.5        0.2      0.398      0.358\n",
      "                 train          4          5      0.571        0.8      0.775      0.348\n",
      "                 truck          5         11          0          0          0          0\n",
      "                  boat          1          1          0          0          0          0\n",
      "         traffic light          4         10          0          0          0          0\n",
      "          fire hydrant          1          1          0          0          0          0\n",
      "             stop sign          1          1          0          0          0          0\n",
      "         parking meter          1          1          0          0          0          0\n",
      "                 bench          5          9          0          0          0          0\n",
      "                  bird          5         24          0          0          0          0\n",
      "                   cat          3          3        0.5      0.333      0.333        0.1\n",
      "                   dog          4          4          0          0          0          0\n",
      "                 horse          2          6          1        0.5       0.75      0.469\n",
      "                 sheep          2          7          0          0          0          0\n",
      "                   cow          4         15          1     0.0667      0.533      0.427\n",
      "              elephant          1          1          0          0          0          0\n",
      "                  bear          1          1          0          0          0          0\n",
      "                 zebra          2          5      0.333        0.2       0.33      0.132\n",
      "               giraffe          2         15          0          0          0          0\n",
      "              backpack          5          9          0          0          0          0\n",
      "              umbrella          3          3          0          0          0          0\n",
      "               handbag          5          9          0          0          0          0\n",
      "                   tie          1          1          0          0          0          0\n",
      "              suitcase          1          5          0          0          0          0\n",
      "               frisbee          1          1          0          0          0          0\n",
      "                  skis          1          4          0          0          0          0\n",
      "           sports ball          7          8          0          0          0          0\n",
      "                  kite          4         16          0          0          0          0\n",
      "          baseball bat          5          9          0          0          0          0\n",
      "        baseball glove          5          7          0          0          0          0\n",
      "             surfboard          4          5          0          0          0          0\n",
      "         tennis racket          2          3          0          0          0          0\n",
      "                bottle         12         35          0          0          0          0\n",
      "            wine glass          4          7          0          0          0          0\n",
      "                   cup          9         26          1     0.0385      0.519      0.415\n",
      "                  fork          1          2          0          0          0          0\n",
      "                 knife          3          3          0          0          0          0\n",
      "                 spoon          7         12          0          0          0          0\n",
      "                  bowl         10         18      0.333     0.0556      0.213       0.17\n",
      "                banana          4         17          0          0          0          0\n",
      "                 apple          1          4          0          0          0          0\n",
      "              sandwich          1          2          0          0          0          0\n",
      "                orange          1          1          0          0          0          0\n",
      "              broccoli          1          2          0          0          0          0\n",
      "                carrot          2          5          0          0          0          0\n",
      "               hot dog          4         10          0          0          0          0\n",
      "                 pizza          5          5          1        0.2        0.6       0.18\n",
      "                 donut          1          1          0          0          0          0\n",
      "                  cake          5         11          0          0          0          0\n",
      "                 chair         15         57      0.161      0.263      0.176     0.0862\n",
      "                 couch          4          4          0          0          0          0\n",
      "          potted plant          6          9          0          0          0          0\n",
      "                   bed          3          3          0          0          0          0\n",
      "          dining table         10         14    0.00419      0.643      0.258     0.0882\n",
      "                toilet          3          3          0          0          0          0\n",
      "                    tv          5          7          0          0          0          0\n",
      "                laptop          3          5          0          0          0          0\n",
      "                 mouse          3          3          0          0          0          0\n",
      "                remote          3          6          0          0          0          0\n",
      "              keyboard          3          4          0          0          0          0\n",
      "            cell phone          6          7          0          0          0          0\n",
      "             microwave          3          3          0          0          0          0\n",
      "                  oven          5          7          0          0          0          0\n",
      "               toaster          1          1          0          0          0          0\n",
      "                  sink          6          7          1      0.143      0.571      0.229\n",
      "          refrigerator          4         12          0          0          0          0\n",
      "                  book          2          2          0          0          0          0\n",
      "                 clock          2          2          0          0          0          0\n",
      "                  vase          6          7          0          0          0          0\n",
      "              scissors          1          1          0          0          0          0\n",
      "            teddy bear          2          2          0          0          0          0\n",
      "Speed: 0.2ms preprocess, 2.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/home/santana/MyStuff/assignments/dl-final-project/trained_models/yolo/yolov8s_640_subset2\u001b[0m\n",
      "Ultralytics 8.3.231 üöÄ Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4060, 7814MiB)\n",
      "Model summary (fused): 72 layers, 11,156,544 parameters, 0 gradients, 28.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 7497.1¬±2267.8 MB/s, size: 167.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/ssd2/santana-coco/data/coco_subset_500_100/labels/val.cache... 100 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 100/100 590.7Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7/7 8.9it/s 0.8s0.1s\n",
      "                   all        100        860      0.106      0.056     0.0833     0.0452\n",
      "                person         54        288    0.00775      0.743       0.44      0.229\n",
      "               bicycle          2          2          0          0          0          0\n",
      "                   car         11         32      0.667      0.125      0.405      0.199\n",
      "            motorcycle          1          2          0          0          0          0\n",
      "              airplane          3          4          0          0          0          0\n",
      "                   bus          3          5        0.5        0.2      0.398      0.358\n",
      "                 train          4          5      0.571        0.8      0.775      0.348\n",
      "                 truck          5         11          0          0          0          0\n",
      "                  boat          1          1          0          0          0          0\n",
      "         traffic light          4         10          0          0          0          0\n",
      "          fire hydrant          1          1          0          0          0          0\n",
      "             stop sign          1          1          0          0          0          0\n",
      "         parking meter          1          1          0          0          0          0\n",
      "                 bench          5          9          0          0          0          0\n",
      "                  bird          5         24          0          0          0          0\n",
      "                   cat          3          3        0.5      0.333      0.333        0.1\n",
      "                   dog          4          4          0          0          0          0\n",
      "                 horse          2          6          1        0.5       0.75      0.469\n",
      "                 sheep          2          7          0          0          0          0\n",
      "                   cow          4         15          1     0.0667      0.533      0.427\n",
      "              elephant          1          1          0          0          0          0\n",
      "                  bear          1          1          0          0          0          0\n",
      "                 zebra          2          5      0.333        0.2       0.33      0.132\n",
      "               giraffe          2         15          0          0          0          0\n",
      "              backpack          5          9          0          0          0          0\n",
      "              umbrella          3          3          0          0          0          0\n",
      "               handbag          5          9          0          0          0          0\n",
      "                   tie          1          1          0          0          0          0\n",
      "              suitcase          1          5          0          0          0          0\n",
      "               frisbee          1          1          0          0          0          0\n",
      "                  skis          1          4          0          0          0          0\n",
      "           sports ball          7          8          0          0          0          0\n",
      "                  kite          4         16          0          0          0          0\n",
      "          baseball bat          5          9          0          0          0          0\n",
      "        baseball glove          5          7          0          0          0          0\n",
      "             surfboard          4          5          0          0          0          0\n",
      "         tennis racket          2          3          0          0          0          0\n",
      "                bottle         12         35          0          0          0          0\n",
      "            wine glass          4          7          0          0          0          0\n",
      "                   cup          9         26          1     0.0385      0.519      0.415\n",
      "                  fork          1          2          0          0          0          0\n",
      "                 knife          3          3          0          0          0          0\n",
      "                 spoon          7         12          0          0          0          0\n",
      "                  bowl         10         18      0.333     0.0556      0.213       0.17\n",
      "                banana          4         17          0          0          0          0\n",
      "                 apple          1          4          0          0          0          0\n",
      "              sandwich          1          2          0          0          0          0\n",
      "                orange          1          1          0          0          0          0\n",
      "              broccoli          1          2          0          0          0          0\n",
      "                carrot          2          5          0          0          0          0\n",
      "               hot dog          4         10          0          0          0          0\n",
      "                 pizza          5          5          1        0.2        0.6       0.18\n",
      "                 donut          1          1          0          0          0          0\n",
      "                  cake          5         11          0          0          0          0\n",
      "                 chair         15         57       0.17      0.281      0.195     0.0904\n",
      "                 couch          4          4          0          0          0          0\n",
      "          potted plant          6          9          0          0          0          0\n",
      "                   bed          3          3          0          0          0          0\n",
      "          dining table         10         14     0.0037      0.571      0.267     0.0884\n",
      "                toilet          3          3          0          0          0          0\n",
      "                    tv          5          7          0          0          0          0\n",
      "                laptop          3          5          0          0          0          0\n",
      "                 mouse          3          3          0          0          0          0\n",
      "                remote          3          6          0          0          0          0\n",
      "              keyboard          3          4          0          0          0          0\n",
      "            cell phone          6          7          0          0          0          0\n",
      "             microwave          3          3          0          0          0          0\n",
      "                  oven          5          7          0          0          0          0\n",
      "               toaster          1          1          0          0          0          0\n",
      "                  sink          6          7          1      0.143      0.571      0.229\n",
      "          refrigerator          4         12          0          0          0          0\n",
      "                  book          2          2          0          0          0          0\n",
      "                 clock          2          2          0          0          0          0\n",
      "                  vase          6          7          0          0          0          0\n",
      "              scissors          1          1          0          0          0          0\n",
      "            teddy bear          2          2          0          0          0          0\n",
      "Speed: 1.0ms preprocess, 4.3ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/santana/MyStuff/assignments/dl-final-project/runs/detect/val5\u001b[0m\n",
      "YOLOv8s fine-tuned: {'stage': 'finetuned', 'map': 0.04519744378343207, 'map50': 0.0832798265202296, 'map75': 0.03633066853811496, 'per_class_map': [0.22934505294812219, 0.0, 0.19886628317177815, 0.0, 0.0, 0.35775, 0.3482619047619048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09999750000000003, 0.0, 0.4692740000000001, 0.0, 0.4266571428571429, 0.0, 0.0, 0.13200000000000003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04519744378343207, 0.0, 0.0, 0.0, 0.0, 0.04519744378343207, 0.0, 0.0, 0.0, 0.0, 0.4153792000000001, 0.0, 0.0, 0.0, 0.17007058823529414, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18, 0.0, 0.0, 0.09043336642501096, 0.0, 0.0, 0.0, 0.08840402247491744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22856666666666675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04519744378343207, 0.04519744378343207]}\n",
      "Saved YOLO history to results/yolo_history.json\n",
      "Copied YOLO results.csv to results/yolov8_results.csv\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# ======================================\n",
    "# 2. YOLOv8: train + val (Ultralytics)\n",
    "# ======================================\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Use the same subset COCO yaml you built earlier\n",
    "# (assumed to point to subset train/val COCO folders)\n",
    "yolo_model = YOLO(\"yolov8s.pt\")  # pretrained on full COCO\n",
    "\n",
    "yolo_history = []\n",
    "\n",
    "# Optional: evaluate pretrained YOLO before fine-tuning on your subset\n",
    "print(\"Evaluating YOLOv8s (pretrained) on coco_subset val...\")\n",
    "metrics0 = yolo_model.val(data=\"data/coco_subset.yaml\", split=\"val\")\n",
    "pre_entry = {\n",
    "    \"stage\": \"pretrained\",\n",
    "    \"map\":   float(metrics0.box.map),\n",
    "    \"map50\": float(metrics0.box.map50),\n",
    "    \"map75\": float(metrics0.box.map75),\n",
    "}\n",
    "yolo_history.append(pre_entry)\n",
    "print(\"YOLOv8s initial:\", pre_entry)\n",
    "\n",
    "# Train on subset COCO\n",
    "yolo_results = yolo_model.train(\n",
    "    data=\"data/coco_subset.yaml\",\n",
    "    epochs=3,\n",
    "    imgsz=640,\n",
    "    batch=16,          # shrink if OOM\n",
    "    device=0,          # or \"cuda:0\"\n",
    "    workers=4,\n",
    "    project=str(MODELS_DIR / \"yolo\"),\n",
    "    name=\"yolov8s_640_subset\",\n",
    ")\n",
    "\n",
    "# Validation on subset val (consistent with proposal setup)\n",
    "metrics = yolo_model.val(data=\"data/coco_subset.yaml\", split=\"val\")\n",
    "post_entry = {\n",
    "    \"stage\": \"finetuned\",\n",
    "    \"map\":   float(metrics.box.map),\n",
    "    \"map50\": float(metrics.box.map50),\n",
    "    \"map75\": float(metrics.box.map75),\n",
    "}\n",
    "\n",
    "# per-class mAP50-95 (for extra analysis if needed)\n",
    "try:\n",
    "    post_entry[\"per_class_map\"] = [float(x) for x in metrics.box.maps]\n",
    "except Exception:\n",
    "    post_entry[\"per_class_map\"] = None\n",
    "\n",
    "yolo_history.append(post_entry)\n",
    "print(\"YOLOv8s fine-tuned:\", post_entry)\n",
    "\n",
    "# Save YOLO history JSON for plotting\n",
    "with open(RESULTS_DIR / \"yolo_history.json\", \"w\") as f:\n",
    "    json.dump(yolo_history, f, indent=2)\n",
    "print(\"Saved YOLO history to\", RESULTS_DIR / \"yolo_history.json\")\n",
    "\n",
    "# Copy Ultralytics training results.csv into results/ for easy loading\n",
    "yolo_run_dir = MODELS_DIR / \"yolo\" / \"yolov8s_640_subset\"\n",
    "yolo_results_csv = yolo_run_dir / \"results.csv\"\n",
    "if yolo_results_csv.exists():\n",
    "    shutil.copy2(yolo_results_csv, RESULTS_DIR / \"yolov8_results.csv\")\n",
    "    print(\"Copied YOLO results.csv to\", RESULTS_DIR / \"yolov8_results.csv\")\n",
    "else:\n",
    "    print(\"WARNING: YOLO results.csv not found at\", yolo_results_csv)\n",
    "\n",
    "# Best YOLO weights are already in:\n",
    "#   trained_models/yolo/yolov8s_640_subset/weights/best.pt\n",
    "# which matches your 'trained_models/' requirement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ef48e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santana/MyStuff/assignments/dl-final-project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "loading annotations into memory...\n",
      "Done (t=4.15s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "Evaluating HF-DETR before training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval HF-DETR (COCO mAP):   0%|          | 0/100 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Eval HF-DETR (COCO mAP): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.08s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Initial HF-DETR metrics: {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train HF-DETR: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:55<00:00,  4.53it/s]\n",
      "Val HF-DETR (loss): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.18it/s]\n",
      "Eval HF-DETR (COCO mAP): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.24s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.08s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002\n",
      "[HF-DETR] Epoch 1/3 | train_loss=6.4989 | val_loss=5.7326 | AP=0.0000 | AP50=0.0000 | AP75=0.0000 | APs=0.0000 | APm=0.0000 | APl=0.0000\n",
      "  -> New best DETR model saved (AP=0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train HF-DETR: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:54<00:00,  4.57it/s]\n",
      "Val HF-DETR (loss): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.14it/s]\n",
      "Eval HF-DETR (COCO mAP): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.23s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.08s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.009\n",
      "[HF-DETR] Epoch 2/3 | train_loss=5.2518 | val_loss=5.1431 | AP=0.0001 | AP50=0.0003 | AP75=0.0000 | APs=0.0000 | APm=0.0001 | APl=0.0004\n",
      "  -> New best DETR model saved (AP=0.0001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train HF-DETR: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:54<00:00,  4.59it/s]\n",
      "Val HF-DETR (loss): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.12it/s]\n",
      "Eval HF-DETR (COCO mAP): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.24s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.08s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.009\n",
      "[HF-DETR] Epoch 3/3 | train_loss=5.1185 | val_loss=4.8703 | AP=0.0001 | AP50=0.0004 | AP75=0.0000 | APs=0.0000 | APm=0.0000 | APl=0.0003\n",
      "Saved HF-DETR history to results/detr_hf_history.json\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# ======================================\n",
    "# 3. HF DETR: train + val loss + COCO mAP\n",
    "# ======================================\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from torch.utils.data import Subset  # used in get_img_ids_for_loader\n",
    "\n",
    "from src.utils.coco_utils import make_coco_loaders\n",
    "\n",
    "\n",
    "def get_img_ids_for_loader(loader):\n",
    "    \"\"\"\n",
    "    Build a list img_ids such that:\n",
    "        img_ids[i] = COCO image_id corresponding to loader.dataset[i]\n",
    "\n",
    "    This walks through possible wrappers (Subset, custom DatasetWrapper, etc.)\n",
    "    and recovers the base COCO dataset's id list.\n",
    "    Assumes val_loader is created with shuffle=False.\n",
    "    \"\"\"\n",
    "    ds = loader.dataset\n",
    "    indices = None  # indices in the base dataset\n",
    "    while True:\n",
    "        if isinstance(ds, Subset):\n",
    "            if indices is None:\n",
    "                indices = list(ds.indices)\n",
    "            else:\n",
    "                indices = [indices[i] for i in ds.indices]\n",
    "            ds = ds.dataset\n",
    "            continue\n",
    "\n",
    "        if not hasattr(ds, \"coco\") and hasattr(ds, \"dataset\"):\n",
    "            ds = ds.dataset\n",
    "            continue\n",
    "\n",
    "        break\n",
    "\n",
    "    if not hasattr(ds, \"coco\"):\n",
    "        raise RuntimeError(\n",
    "            \"Could not find a base COCO dataset with a 'coco' attribute under loader.dataset\"\n",
    "        )\n",
    "\n",
    "    base_ds = ds\n",
    "    coco = base_ds.coco\n",
    "\n",
    "    if indices is None:\n",
    "        indices = list(range(len(base_ds)))\n",
    "\n",
    "    if hasattr(base_ds, \"ids\"):\n",
    "        base_img_ids = list(base_ds.ids)\n",
    "    else:\n",
    "        base_img_ids = list(sorted(coco.getImgIds()))\n",
    "\n",
    "    loader_img_ids = [int(base_img_ids[i]) for i in indices]\n",
    "    return loader_img_ids\n",
    "\n",
    "\n",
    "def get_coco_api_from_loader(loader):\n",
    "    \"\"\"\n",
    "    Robustly get the underlying pycocotools COCO object from a DataLoader.\n",
    "    \"\"\"\n",
    "    ds = loader.dataset\n",
    "    for _ in range(10):\n",
    "        if hasattr(ds, \"coco\"):\n",
    "            return ds.coco\n",
    "        if hasattr(ds, \"dataset\"):\n",
    "            ds = ds.dataset\n",
    "        else:\n",
    "            break\n",
    "    raise AttributeError(\n",
    "        \"Could not find 'coco' attribute in dataset. \"\n",
    "        \"Please check make_coco_loaders implementation.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helper: xyxy ‚Üí HF targets\n",
    "# -------------------------\n",
    "def build_hf_targets(targets):\n",
    "    \"\"\"\n",
    "    Convert a batch of targets from your format:\n",
    "        {\n",
    "            \"boxes\": Tensor[num_boxes, 4] in xyxy,\n",
    "            \"labels\": Tensor[num_boxes],\n",
    "            (optionally \"image_id\", \"area\", \"iscrowd\")\n",
    "        }\n",
    "    into HF/COCO-style annotations.\n",
    "    \"\"\"\n",
    "    hf_targets = []\n",
    "\n",
    "    for t in targets:\n",
    "        boxes = t[\"boxes\"]  # (N, 4), xyxy\n",
    "        labels = t[\"labels\"]\n",
    "\n",
    "        if boxes.numel() == 0:\n",
    "            annotations = []\n",
    "        else:\n",
    "            xywh = boxes.clone()\n",
    "            xywh[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "            xywh[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "            annotations = []\n",
    "            for box, label in zip(xywh, labels):\n",
    "                bbox = box.tolist()\n",
    "                category_id = int(label.item() if torch.is_tensor(label) else label)\n",
    "                ann = {\n",
    "                    \"bbox\": bbox,\n",
    "                    \"category_id\": category_id,\n",
    "                    \"area\": float(bbox[2] * bbox[3]),\n",
    "                    \"iscrowd\": 0,\n",
    "                }\n",
    "                annotations.append(ann)\n",
    "\n",
    "        if \"image_id\" in t:\n",
    "            if torch.is_tensor(t[\"image_id\"]):\n",
    "                image_id = int(t[\"image_id\"].item())\n",
    "            else:\n",
    "                image_id = int(t[\"image_id\"])\n",
    "        else:\n",
    "            image_id = 0\n",
    "\n",
    "        hf_targets.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"annotations\": annotations,\n",
    "        })\n",
    "\n",
    "    return hf_targets\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Training loop for HF DETR\n",
    "# -------------------------\n",
    "def train_one_epoch_detr_hf(model, processor, loader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, targets in tqdm(loader, desc=\"Train HF-DETR\"):\n",
    "        hf_targets = build_hf_targets(targets)\n",
    "\n",
    "        encoding = processor(\n",
    "            images=list(images),\n",
    "            annotations=hf_targets,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        pixel_values = encoding[\"pixel_values\"].to(device)\n",
    "        labels = [\n",
    "            {k: v.to(device) for k, v in target.items()}\n",
    "            for target in encoding[\"labels\"]\n",
    "        ]\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_one_epoch_detr_hf(model, processor, loader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, targets in tqdm(loader, desc=\"Val HF-DETR (loss)\"):\n",
    "        hf_targets = build_hf_targets(targets)\n",
    "\n",
    "        encoding = processor(\n",
    "            images=list(images),\n",
    "            annotations=hf_targets,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        pixel_values = encoding[\"pixel_values\"].to(device)\n",
    "        labels = [\n",
    "            {k: v.to(device) for k, v in target.items()}\n",
    "            for target in encoding[\"labels\"]\n",
    "        ]\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# COCO mAP evaluation for HF DETR\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate_coco_mAP_detr(model, processor, loader, device):\n",
    "    \"\"\"\n",
    "    COCO-style evaluation for HF DETR on *your subset* of val2017.\n",
    "\n",
    "    - Ensures image_ids in results match subset used by the DataLoader.\n",
    "    - Maps DETR label indices -> COCO category_id using id2label + coco.getCatIds.\n",
    "    - Restricts COCOeval to only those imgIds we actually predicted on.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    coco = get_coco_api_from_loader(loader)\n",
    "    coco_img_ids_all = set(coco.getImgIds())\n",
    "\n",
    "    loader_img_ids = get_img_ids_for_loader(loader)\n",
    "    assert len(loader_img_ids) == len(loader.dataset), \\\n",
    "        \"Length mismatch between loader_img_ids and loader.dataset\"\n",
    "\n",
    "    id2label = {int(k): v for k, v in model.config.id2label.items()}\n",
    "\n",
    "    label_idx_to_cat_id = {}\n",
    "    for idx, name in id2label.items():\n",
    "        cat_ids = coco.getCatIds(catNms=[name])\n",
    "        if len(cat_ids) > 0:\n",
    "            label_idx_to_cat_id[idx] = cat_ids[0]\n",
    "\n",
    "    if not label_idx_to_cat_id:\n",
    "        print(\"WARNING: could not map any DETR labels to COCO category ids.\")\n",
    "        return None\n",
    "\n",
    "    results = []\n",
    "    global_idx = 0\n",
    "\n",
    "    for images, _targets in tqdm(loader, desc=\"Eval HF-DETR (COCO mAP)\"):\n",
    "        images = list(images)\n",
    "\n",
    "        target_sizes = []\n",
    "        for img in images:\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                h, w = img.shape[-2:]\n",
    "            else:\n",
    "                w, h = img.size\n",
    "            target_sizes.append([h, w])\n",
    "\n",
    "        encoding = processor(images=images, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].to(device)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        processed_outputs = processor.post_process_object_detection(\n",
    "            outputs=outputs,\n",
    "            target_sizes=torch.tensor(target_sizes, device=device),\n",
    "            threshold=0.0,\n",
    "        )\n",
    "\n",
    "        batch_size = len(processed_outputs)\n",
    "        batch_img_ids = loader_img_ids[global_idx: global_idx + batch_size]\n",
    "        global_idx += batch_size\n",
    "\n",
    "        for img_id, pred in zip(batch_img_ids, processed_outputs):\n",
    "            if img_id not in coco_img_ids_all:\n",
    "                continue\n",
    "\n",
    "            boxes = pred[\"boxes\"].detach().cpu()\n",
    "            scores = pred[\"scores\"].detach().cpu()\n",
    "            labels = pred[\"labels\"].detach().cpu()\n",
    "\n",
    "            if boxes.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            boxes_xywh = boxes.clone()\n",
    "            boxes_xywh[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "            boxes_xywh[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "            for box, score, label in zip(boxes_xywh, scores, labels):\n",
    "                label_idx = int(label)\n",
    "                if label_idx not in label_idx_to_cat_id:\n",
    "                    continue\n",
    "\n",
    "                cat_id = int(label_idx_to_cat_id[label_idx])\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"image_id\": int(img_id),\n",
    "                        \"category_id\": cat_id,\n",
    "                        \"bbox\": box.tolist(),\n",
    "                        \"score\": float(score),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if not results:\n",
    "        print(\"No detections to evaluate (results list is empty).\")\n",
    "        return None\n",
    "\n",
    "    coco_dt = coco.loadRes(results)\n",
    "    coco_eval = COCOeval(coco, coco_dt, iouType=\"bbox\")\n",
    "\n",
    "    eval_img_ids = sorted({r[\"image_id\"] for r in results})\n",
    "    coco_eval.params.imgIds = eval_img_ids\n",
    "\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    metrics = {\n",
    "        \"AP\":   float(coco_eval.stats[0]),\n",
    "        \"AP50\": float(coco_eval.stats[1]),\n",
    "        \"AP75\": float(coco_eval.stats[2]),\n",
    "        \"APs\":  float(coco_eval.stats[3]),\n",
    "        \"APm\":  float(coco_eval.stats[4]),\n",
    "        \"APl\":  float(coco_eval.stats[5]),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ------------ main DETR script ------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_loader, val_loader = make_coco_loaders(\n",
    "    root=\"/mnt/ssd2/santana-coco/data/coco\",\n",
    "    batch_size=2,\n",
    "    num_workers=4,\n",
    "    train_limit=500,\n",
    "    val_limit=100,\n",
    ")\n",
    "\n",
    "processor = DetrImageProcessor.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\",\n",
    "    revision=\"no_timm\",\n",
    ")\n",
    "model = DetrForObjectDetection.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\",\n",
    "    revision=\"no_timm\",\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "num_epochs = 3\n",
    "best_ap = 0.0\n",
    "detr_history = []\n",
    "\n",
    "# Optional: evaluate DETR before fine-tuning\n",
    "print(\"Evaluating HF-DETR before training...\")\n",
    "metrics0 = evaluate_coco_mAP_detr(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    loader=val_loader,\n",
    "    device=device,\n",
    ")\n",
    "print(\"Initial HF-DETR metrics:\", metrics0)\n",
    "\n",
    "entry0 = {\"epoch\": 0, \"train_loss\": None, \"val_loss\": None}\n",
    "if metrics0 is not None:\n",
    "    entry0.update(metrics0)\n",
    "detr_history.append(entry0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch_detr_hf(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    val_loss = validate_one_epoch_detr_hf(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        loader=val_loader,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    metrics = evaluate_coco_mAP_detr(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        loader=val_loader,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    if metrics is not None:\n",
    "        ap = metrics[\"AP\"]\n",
    "        print(\n",
    "            f\"[HF-DETR] Epoch {epoch + 1}/{num_epochs} | \"\n",
    "            f\"train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n",
    "            f\"AP={ap:.4f} | AP50={metrics['AP50']:.4f} | \"\n",
    "            f\"AP75={metrics['AP75']:.4f} | APs={metrics['APs']:.4f} | \"\n",
    "            f\"APm={metrics['APm']:.4f} | APl={metrics['APl']:.4f}\"\n",
    "        )\n",
    "\n",
    "        history_entry = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "        }\n",
    "        history_entry.update(metrics)\n",
    "        detr_history.append(history_entry)\n",
    "\n",
    "        if ap > best_ap:\n",
    "            best_ap = ap\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                MODELS_DIR / \"detr_hf_best.pth\",\n",
    "            )\n",
    "            print(f\"  -> New best DETR model saved (AP={ap:.4f})\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"[HF-DETR] Epoch {epoch + 1}/{num_epochs} | \"\n",
    "            f\"train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n",
    "            f\"no detections on val set\"\n",
    "        )\n",
    "        detr_history.append(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        MODELS_DIR / f\"detr_hf_epoch{epoch + 1}.pth\",\n",
    "    )\n",
    "\n",
    "# Save DETR history for plotting\n",
    "with open(RESULTS_DIR / \"detr_hf_history.json\", \"w\") as f:\n",
    "    json.dump(detr_history, f, indent=2)\n",
    "print(\"Saved HF-DETR history to\", RESULTS_DIR / \"detr_hf_history.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
